{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import packages","metadata":{}},{"cell_type":"code","source":"# Numerical Operations\nimport math\nimport numpy as np\n\n# Reading/Writing Data\nimport pandas as pd\nimport os\nimport csv\n\n# For Progress Bar\nfrom tqdm import tqdm\n\n# Pytorch\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\n# Matplotlib\nimport matplotlib.pyplot as plt\n\n# Optuna\nimport optuna\n\n# For plotting learning curve\nfrom torch.utils.tensorboard import SummaryWriter","metadata":{"execution":{"iopub.status.busy":"2023-05-31T06:02:22.568116Z","iopub.execute_input":"2023-05-31T06:02:22.569035Z","iopub.status.idle":"2023-05-31T06:02:34.461806Z","shell.execute_reply.started":"2023-05-31T06:02:22.569003Z","shell.execute_reply":"2023-05-31T06:02:34.460845Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Some Utility Functions\n\nYou do not need to modify this part.","metadata":{}},{"cell_type":"code","source":"def same_seed(seed):\n    '''Fixes random number generator seeds for reproducibility.'''\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\ndef train_valid_split(data_set, valid_ratio, seed):\n    '''Split provided training data into training set and validation set'''\n    valid_set_size = int(valid_ratio * len(data_set))\n    train_set_size = len(data_set) - valid_set_size\n    train_set, valid_set = random_split(data_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(seed))\n    return np.array(train_set), np.array(valid_set)\n\ndef predict(test_loader, model, device):\n    model.eval() # Set your model to evaluation mode.\n    preds = []\n    for x in tqdm(test_loader):\n        x = x.to(device)\n        with torch.no_grad():\n            pred = model(x)\n            preds.append(pred.detach().cpu())\n    preds = torch.cat(preds, dim=0).numpy()\n    return preds","metadata":{"execution":{"iopub.status.busy":"2023-05-31T06:02:34.463776Z","iopub.execute_input":"2023-05-31T06:02:34.464555Z","iopub.status.idle":"2023-05-31T06:02:34.473560Z","shell.execute_reply.started":"2023-05-31T06:02:34.464518Z","shell.execute_reply":"2023-05-31T06:02:34.472562Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class CovidDataset(Dataset):\n    def __init__(self, x, y=None):\n        if y is None:\n            self.y = y\n        else:\n            self.y = torch.FloatTensor(y)\n        self.x = torch.FloatTensor(x)\n    \n    def __getitem__(self, idx):\n        if self.y is None:\n            return self.x[idx]\n        else:\n            return self.x[idx], self.y[idx]\n    \n    def __len__(self):\n        return len(self.x)","metadata":{"execution":{"iopub.status.busy":"2023-05-31T06:02:34.475247Z","iopub.execute_input":"2023-05-31T06:02:34.475933Z","iopub.status.idle":"2023-05-31T06:02:34.484713Z","shell.execute_reply.started":"2023-05-31T06:02:34.475900Z","shell.execute_reply":"2023-05-31T06:02:34.483814Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Neural Network Model\n\nTry out different model architectures by modifying the class below. (You could tune config['layer'] to try)","metadata":{}},{"cell_type":"code","source":"class My_Model(nn.Module):\n    def __init__(self, input_dim):\n        super(My_Model, self).__init__()\n        # TODO: modify model's structure, be aware of dimensions.\n        self.layers = nn.Sequential(\n            nn.Linear(input_dim, config['layer'][0]),\n            nn.ReLU(),\n            nn.Linear(config['layer'][0], config['layer'][1]),\n            nn.ReLU(),\n            nn.Linear(config['layer'][1], 1)\n        )\n\n    def forward(self, x):\n        x = self.layers(x)\n        x = x.squeeze(1) # (B, 1) -> (B)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-05-31T06:02:34.487569Z","iopub.execute_input":"2023-05-31T06:02:34.487925Z","iopub.status.idle":"2023-05-31T06:02:34.496697Z","shell.execute_reply.started":"2023-05-31T06:02:34.487893Z","shell.execute_reply":"2023-05-31T06:02:34.495778Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Feature Selection\n\nChoose features you deem useful by modifying the function below.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, f_regression\n\ndef select_feat(train_data, valid_data, test_data, no_select_all=True):\n    '''Selects useful features to perform regression'''\n    global config\n    y_train, y_valid = train_data[:,-1], valid_data[:,-1]\n    raw_x_train, raw_x_valid, raw_x_test = train_data[:,:-1], valid_data[:,:-1], test_data\n\n    if not no_select_all:\n        feat_idx = list(range(raw_x_train.shape[1]))\n    else:\n        # feature selection\n        k = config['k']\n        selector = SelectKBest(score_func=f_regression, k=k)\n        result = selector.fit(train_data[:, :-1], train_data[:,-1])\n        idx = np.argsort(result.scores_)[::-1]\n        feat_idx = list(np.sort(idx[:k]))\n\n    return raw_x_train[:,feat_idx], raw_x_valid[:,feat_idx], raw_x_test[:,feat_idx], y_train, y_valid","metadata":{"execution":{"iopub.status.busy":"2023-05-31T06:02:34.497985Z","iopub.execute_input":"2023-05-31T06:02:34.498453Z","iopub.status.idle":"2023-05-31T06:02:35.061842Z","shell.execute_reply.started":"2023-05-31T06:02:34.498417Z","shell.execute_reply":"2023-05-31T06:02:35.060912Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Training Loop","metadata":{}},{"cell_type":"code","source":"def trainer(train_loader, valid_loader, model, config, device):\n\n    criterion = nn.MSELoss(reduction='mean') # Define your loss function, do not modify this.\n    \n    # Define your optimization algorithm.\n    if config['optim'] == 'SGD':\n        if config['no_momentum']:\n            optimizer = torch.optim.SGD(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])     \n        else:\n            optimizer = torch.optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=config['momentum'], weight_decay=config['weight_decay'])     \n    elif config['optim'] == 'Adam':\n        optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n        \n    writer = SummaryWriter() # Writer of tensoboard.\n\n        \n    if not os.path.isdir('./models'):\n        os.mkdir('./models') # Create directory of saving models.\n\n    n_epochs, best_loss, step, early_stop_count = config['n_epochs'], math.inf, 0, 0\n\n    for epoch in range(n_epochs):\n        model.train() # Set your model to train mode.\n        loss_record = []\n        \n        # 如果你在kaggle上运行，可以注释掉大部分的打印函数，并将train_pbar注释掉，令 x,y in train_loader，因为kaggle上打印太多可能会报错。\n        # tqdm is a package to visualize your training progress.\n#         train_pbar = tqdm(train_loader, position=0, leave=True)\n\n#         for x, y in train_pbar:\n        for x, y in train_loader:\n            optimizer.zero_grad()               # Set gradient to zero.\n            x, y = x.to(device), y.to(device)   # Move your data to device.\n            pred = model(x)\n            loss = criterion(pred, y)\n            loss.backward()                     # Compute gradient(backpropagation).\n            optimizer.step()                    # Update parameters.\n            step += 1\n            loss_record.append(loss.detach().item())\n\n            # Display current epoch number and loss on tqdm progress bar.\n#             train_pbar.set_description(f'Epoch [{epoch+1}/{n_epochs}]')\n#             train_pbar.set_postfix({'loss': loss.detach().item()})\n\n        mean_train_loss = sum(loss_record)/len(loss_record)\n\n        model.eval() # Set your model to evaluation mode.\n        loss_record = []\n        for x, y in valid_loader:\n            x, y = x.to(device), y.to(device)\n            with torch.no_grad():\n                pred = model(x)\n                loss = criterion(pred, y)\n\n            loss_record.append(loss.item())\n\n        mean_valid_loss = sum(loss_record)/len(loss_record)        \n        \n#         if epoch % 100 == 0:\n#             print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}')\n\n        if not config['no_tensorboard']:\n            writer.add_scalar('Loss/train', mean_train_loss, step)\n            writer.add_scalar('Loss/valid', mean_valid_loss, step)\n\n        if mean_valid_loss < best_loss:\n            best_loss = mean_valid_loss\n            \n            # 一轮实验中保存 K 折交叉验证中单折表现最好的模型\n            if len(valid_scores):\n                if best_loss < min(valid_scores):\n                    torch.save(model.state_dict(), config['save_path']) # Save your best model\n#                     print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}')\n                    print('Saving model with loss {:.3f}...'.format(best_loss))\n            else:\n                torch.save(model.state_dict(), config['save_path']) # Save your best model\n#                 print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}')\n                print('Saving model with loss {:.3f}...'.format(best_loss))\n                \n            early_stop_count = 0\n        else:\n            early_stop_count += 1\n\n        if early_stop_count >= config['early_stop']:\n            print('Best loss {:.3f}...'.format(best_loss))\n            print('\\nModel is not improving, so we halt the training session.')\n            break\n    return best_loss","metadata":{"execution":{"iopub.status.busy":"2023-05-31T06:02:35.063296Z","iopub.execute_input":"2023-05-31T06:02:35.063794Z","iopub.status.idle":"2023-05-31T06:02:35.080938Z","shell.execute_reply.started":"2023-05-31T06:02:35.063760Z","shell.execute_reply":"2023-05-31T06:02:35.079707Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Save predictions","metadata":{}},{"cell_type":"code","source":"def save_pred(preds, file):\n    ''' Save predictions to specified file '''\n    with open(file, 'w') as fp:\n        writer = csv.writer(fp)\n        writer.writerow(['id', 'tested_positive'])\n        for i, p in enumerate(preds):\n            writer.writerow([i, p])","metadata":{"execution":{"iopub.status.busy":"2023-05-31T06:02:35.083689Z","iopub.execute_input":"2023-05-31T06:02:35.084092Z","iopub.status.idle":"2023-05-31T06:02:35.095584Z","shell.execute_reply.started":"2023-05-31T06:02:35.084066Z","shell.execute_reply":"2023-05-31T06:02:35.094673Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\n# dropbox link\n!wget -O covid_train.csv https://www.dropbox.com/s/lmy1riadzoy0ahw/covid.train.csv?dl=0\n!wget -O covid_test.csv https://www.dropbox.com/s/zalbw42lu4nmhr2/covid.test.csv?dl=0","metadata":{"execution":{"iopub.status.busy":"2023-05-31T06:02:35.097157Z","iopub.execute_input":"2023-05-31T06:02:35.097611Z","iopub.status.idle":"2023-05-31T06:02:39.209149Z","shell.execute_reply.started":"2023-05-31T06:02:35.097579Z","shell.execute_reply":"2023-05-31T06:02:39.208046Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"--2023-05-31 06:02:35--  https://www.dropbox.com/s/lmy1riadzoy0ahw/covid.train.csv?dl=0\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6016:18::a27d:112\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: /s/raw/lmy1riadzoy0ahw/covid.train.csv [following]\n--2023-05-31 06:02:36--  https://www.dropbox.com/s/raw/lmy1riadzoy0ahw/covid.train.csv\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://uc3bc52db62bdf683bca083f89c3.dl.dropboxusercontent.com/cd/0/inline/B9HQRetIW3Qmui02z0D9aDBxfFRehcWiDOo5foWKZ1BMj8Yl_pip1wJdK6bd2nHPCbhB0pLU5FE6aAx_U11i6KSKnZDSpR_7k07TqEDa2AXOgM7LFgnRtlyKIjpv4sCczWwKc5Y_sFvEUT7wD4-SjyNcA021ozTsrtyGpUf9EA_MHg/file# [following]\n--2023-05-31 06:02:36--  https://uc3bc52db62bdf683bca083f89c3.dl.dropboxusercontent.com/cd/0/inline/B9HQRetIW3Qmui02z0D9aDBxfFRehcWiDOo5foWKZ1BMj8Yl_pip1wJdK6bd2nHPCbhB0pLU5FE6aAx_U11i6KSKnZDSpR_7k07TqEDa2AXOgM7LFgnRtlyKIjpv4sCczWwKc5Y_sFvEUT7wD4-SjyNcA021ozTsrtyGpUf9EA_MHg/file\nResolving uc3bc52db62bdf683bca083f89c3.dl.dropboxusercontent.com (uc3bc52db62bdf683bca083f89c3.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\nConnecting to uc3bc52db62bdf683bca083f89c3.dl.dropboxusercontent.com (uc3bc52db62bdf683bca083f89c3.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2162766 (2.1M) [text/plain]\nSaving to: ‘covid_train.csv’\n\ncovid_train.csv     100%[===================>]   2.06M  --.-KB/s    in 0.04s   \n\n2023-05-31 06:02:37 (51.0 MB/s) - ‘covid_train.csv’ saved [2162766/2162766]\n\n--2023-05-31 06:02:38--  https://www.dropbox.com/s/zalbw42lu4nmhr2/covid.test.csv?dl=0\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6016:18::a27d:112\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: /s/raw/zalbw42lu4nmhr2/covid.test.csv [following]\n--2023-05-31 06:02:38--  https://www.dropbox.com/s/raw/zalbw42lu4nmhr2/covid.test.csv\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://ucf641e25fdc452dbaccf05cafe6.dl.dropboxusercontent.com/cd/0/inline/B9HAPJvVel_a2n0K8QNETvAOxUHhDX-92gR2_9jnMs7QVIVXwvpMHPnuPQo_2eA9mxh749h4VxCnacEhYK3nEXgufCafsa15nipWZjAVTqX8RyMdoK2FqYuwr-D0dYZmGn8-bZ9SvQxn-foC6pf6LzuK-_y5qSDdnNva1iGOvkg4zw/file# [following]\n--2023-05-31 06:02:38--  https://ucf641e25fdc452dbaccf05cafe6.dl.dropboxusercontent.com/cd/0/inline/B9HAPJvVel_a2n0K8QNETvAOxUHhDX-92gR2_9jnMs7QVIVXwvpMHPnuPQo_2eA9mxh749h4VxCnacEhYK3nEXgufCafsa15nipWZjAVTqX8RyMdoK2FqYuwr-D0dYZmGn8-bZ9SvQxn-foC6pf6LzuK-_y5qSDdnNva1iGOvkg4zw/file\nResolving ucf641e25fdc452dbaccf05cafe6.dl.dropboxusercontent.com (ucf641e25fdc452dbaccf05cafe6.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\nConnecting to ucf641e25fdc452dbaccf05cafe6.dl.dropboxusercontent.com (ucf641e25fdc452dbaccf05cafe6.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 638359 (623K) [text/plain]\nSaving to: ‘covid_test.csv’\n\ncovid_test.csv      100%[===================>] 623.40K  --.-KB/s    in 0.04s   \n\n2023-05-31 06:02:39 (15.4 MB/s) - ‘covid_test.csv’ saved [638359/638359]\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Start training!\n\nconfig contains hyper-parameters for training and the path to save your model.\n\n`objective()` is used for automatic parameter tuning, but you could set `AUTO_TUNE_PARAM` `False` to avoid it.","metadata":{}},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nif torch.cuda.is_available():\n    print(\"OKGPU\")\n\nconfig = {\n    'seed': 5201314,      # Your seed number, you can pick your lucky number. :)\n    'k': 16,    # Select k feature\n    'layer': [16, 8],\n    'optim': 'Adam',\n    'momentum': 0.7,\n    'valid_ratio': 0.2,   # validation_size = train_size * valid_ratio\n    'n_epochs': 10000,     # Number of epochs.\n    'batch_size': 128,\n    'learning_rate': 1e-03,\n    'weight_decay': 1e-4,\n    'early_stop': 600,    # If model has not improved for this many consecutive epochs, stop training.\n    'save_path': './models/model.ckpt',  # Your model will be saved here.\n    'no_select_all': True,   # Whether to use all features.\n    'no_momentum': False,   # Whether to use momentum\n    'no_normal': False,  # Whether to normalize data\n    'no_k_cross': False,     # Whether to use K-fold cross validation\n    'no_save': False,   # Whether to save model parameters\n    'no_tensorboard': True,  # Whether to write tensorboard\n} \n\n# 设置 k-fold 中的 k，这里是根据 valid_ratio 设定的\nk = int(1 / config['valid_ratio'])\n\n # Set seed for reproducibility\nsame_seed(config['seed'])\n\ntraining_data, test_data = pd.read_csv('/kaggle/input/ml2023spring-hw1/covid_train.csv').values, pd.read_csv('/kaggle/input/ml2023spring-hw1/covid_test.csv').values\n    \nnum_valid_samples = len(training_data) // k\nnp.random.shuffle(training_data)\nvalid_scores = []  # 记录 valid_loss\n\ndef objective(trial):\n    if trial != None:\n        print('\\nNew trial here')\n        # 定义需要调优的超参数空间\n        config['seed'] = trial.suggest_int('seed', 1, 520131455)\n        # config['learning_rate'] = trial.suggest_float('lr', 1e-5, 1e-4)\n        # config['batch_size'] = trial.suggest_categorical('batch_size', [128])\n        # config['k'] = trial.suggest_int('k_feats', 32, 40)\n        config['layer'][0] = config['k']\n    \n    # 打印所需的超参数\n    print(f'''hyper-parameter: \n        seed: {config['seed']}\n        optimizer: {config['optim']},\n        lr: {config['learning_rate']}, \n        batch_size: {config['batch_size']}, \n        k: {config['k']}, \n        layer: {config['layer']}''')\n    \n    global valid_scores\n    # 每次 trial 初始化 valid_scores，可以不初始化，通过 trial * k + fold 来访问当前 trial 的 valid_score，\n    # 这样可以让 trainer() 保存 trials 中最好的模型参数，但这并不意味着该参数对应的 k-fold validation loss 最低。\n    valid_scores = []\n\n    for fold in range(k):\n        # Data split\n        valid_data = training_data[num_valid_samples * fold:\n                                num_valid_samples * (fold + 1)]\n        train_data = np.concatenate((\n            training_data[:num_valid_samples * fold],\n            training_data[num_valid_samples * (fold + 1):]))\n\n        # Normalization\n        if not config['no_normal']:\n            train_mean = np.mean(train_data[:, 35:-1], axis=0)  # 前 35 列为 one-hot vector，我并没有对他们做 normalization，可以自行设置\n            train_std = np.std(train_data[:, 35:-1], axis=0)\n            train_data[:, 35:-1] -= train_mean\n            train_data[:, 35:-1] /= train_std\n            valid_data[:, 35:-1] -= train_mean\n            valid_data[:, 35:-1] /= train_std\n            test_data[:, 35:] -= train_mean\n            test_data[:, 35:] /= train_std\n\n        x_train, x_valid, x_test, y_train, y_valid = select_feat(train_data, valid_data, test_data, config['no_select_all'])\n        \n        train_dataset, valid_dataset, test_dataset = CovidDataset(x_train, y_train), \\\n                                                CovidDataset(x_valid, y_valid), \\\n                                                CovidDataset(x_test)\n\n        # Pytorch data loader loads pytorch dataset into batches.\n        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\n        valid_loader = DataLoader(valid_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\n        test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, pin_memory=True)\n        \n        model = My_Model(input_dim=x_train.shape[1]).to(device) # put your model and data on the same computation device.\n        valid_score = trainer(train_loader, valid_loader, model, config, device)\n        valid_scores.append(valid_score)\n        \n        if not config['no_k_cross']:\n            break\n            \n        if valid_score > 2:\n            print(f'在第{fold+1}折上欠拟合') # 提前终止，减少计算资源\n            break       \n        \n    print(f'valid_scores: {valid_scores}')\n    \n    if trial != None:\n        return np.average(valid_scores)\n    else:\n        return x_test, test_loader\n\n\nAUTO_TUNE_PARAM = False  # Whether to tune parameters automatically\n\nif AUTO_TUNE_PARAM:\n    # 使用Optuna库进行超参数搜索\n    n_trials = 50  # 设置试验数量\n    print(f'AUTO_TUNE_PARAM: {AUTO_TUNE_PARAM}\\nn_trials: {n_trials}')\n    study = optuna.create_study(direction='minimize')\n    study.optimize(objective, n_trials=n_trials)\n\n    # 输出最优的超参数组合和性能指标\n    print('Best hyperparameters: {}'.format(study.best_params))\n    print('Best performance: {:.4f}'.format(study.best_value))\nelse:\n    # 注意，只有非自动调参时才进行了predict，节省一下计算资源\n    print(f'You could set AUTO_TUNE_PARAM True to tune parameters automatically.\\nAUTO_TUNE_PARAM: {AUTO_TUNE_PARAM}')\n    x_test, test_loader = objective(None)\n    model = My_Model(input_dim=x_test.shape[1]).to(device)\n    model.load_state_dict(torch.load(config['save_path']))\n    preds = predict(test_loader, model, device)\n    save_pred(preds, 'submission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-05-31T06:12:56.071540Z","iopub.execute_input":"2023-05-31T06:12:56.071902Z","iopub.status.idle":"2023-05-31T06:13:44.864658Z","shell.execute_reply.started":"2023-05-31T06:12:56.071871Z","shell.execute_reply":"2023-05-31T06:13:44.863696Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"OKGPU\nYou could set AUTO_TUNE_PARAM True to tune parameters automatically.\nAUTO_TUNE_PARAM: False\nhyper-parameter: \n        seed: 5201314\n        optimizer: Adam,\n        lr: 0.001, \n        batch_size: 128, \n        k: 16, \n        layer: [16, 8]\nSaving model with loss 384.245...\nSaving model with loss 378.793...\nSaving model with loss 376.269...\nSaving model with loss 375.082...\nSaving model with loss 372.826...\nSaving model with loss 362.742...\nSaving model with loss 349.744...\nSaving model with loss 329.850...\nSaving model with loss 305.995...\nSaving model with loss 280.810...\nSaving model with loss 244.356...\nSaving model with loss 210.668...\nSaving model with loss 174.213...\nSaving model with loss 144.797...\nSaving model with loss 119.949...\nSaving model with loss 104.996...\nSaving model with loss 96.262...\nSaving model with loss 90.725...\nSaving model with loss 88.071...\nSaving model with loss 85.631...\nSaving model with loss 84.356...\nSaving model with loss 81.427...\nSaving model with loss 81.106...\nSaving model with loss 78.137...\nSaving model with loss 75.367...\nSaving model with loss 73.465...\nSaving model with loss 71.301...\nSaving model with loss 68.700...\nSaving model with loss 66.692...\nSaving model with loss 64.265...\nSaving model with loss 61.556...\nSaving model with loss 59.062...\nSaving model with loss 55.473...\nSaving model with loss 52.835...\nSaving model with loss 49.880...\nSaving model with loss 47.036...\nSaving model with loss 43.808...\nSaving model with loss 40.983...\nSaving model with loss 37.478...\nSaving model with loss 34.522...\nSaving model with loss 31.155...\nSaving model with loss 28.669...\nSaving model with loss 25.994...\nSaving model with loss 23.916...\nSaving model with loss 22.108...\nSaving model with loss 20.615...\nSaving model with loss 19.622...\nSaving model with loss 18.102...\nSaving model with loss 16.997...\nSaving model with loss 16.162...\nSaving model with loss 15.619...\nSaving model with loss 15.325...\nSaving model with loss 14.484...\nSaving model with loss 14.223...\nSaving model with loss 13.579...\nSaving model with loss 13.026...\nSaving model with loss 12.483...\nSaving model with loss 12.298...\nSaving model with loss 11.582...\nSaving model with loss 11.172...\nSaving model with loss 10.692...\nSaving model with loss 10.230...\nSaving model with loss 9.643...\nSaving model with loss 9.550...\nSaving model with loss 9.012...\nSaving model with loss 8.495...\nSaving model with loss 8.195...\nSaving model with loss 7.716...\nSaving model with loss 7.511...\nSaving model with loss 7.155...\nSaving model with loss 6.672...\nSaving model with loss 6.417...\nSaving model with loss 6.038...\nSaving model with loss 5.786...\nSaving model with loss 5.467...\nSaving model with loss 5.166...\nSaving model with loss 4.955...\nSaving model with loss 4.660...\nSaving model with loss 4.383...\nSaving model with loss 4.191...\nSaving model with loss 3.989...\nSaving model with loss 3.790...\nSaving model with loss 3.600...\nSaving model with loss 3.369...\nSaving model with loss 3.211...\nSaving model with loss 3.070...\nSaving model with loss 2.841...\nSaving model with loss 2.723...\nSaving model with loss 2.595...\nSaving model with loss 2.432...\nSaving model with loss 2.327...\nSaving model with loss 2.248...\nSaving model with loss 2.076...\nSaving model with loss 2.029...\nSaving model with loss 1.912...\nSaving model with loss 1.853...\nSaving model with loss 1.783...\nSaving model with loss 1.680...\nSaving model with loss 1.599...\nSaving model with loss 1.573...\nSaving model with loss 1.554...\nSaving model with loss 1.510...\nSaving model with loss 1.458...\nSaving model with loss 1.420...\nSaving model with loss 1.394...\nSaving model with loss 1.385...\nSaving model with loss 1.328...\nSaving model with loss 1.268...\nSaving model with loss 1.257...\nSaving model with loss 1.236...\nSaving model with loss 1.194...\nSaving model with loss 1.167...\nSaving model with loss 1.138...\nSaving model with loss 1.135...\nSaving model with loss 1.121...\nSaving model with loss 1.115...\nSaving model with loss 1.083...\nSaving model with loss 1.072...\nSaving model with loss 1.050...\nSaving model with loss 1.036...\nSaving model with loss 1.028...\nSaving model with loss 1.009...\nSaving model with loss 0.998...\nSaving model with loss 0.982...\nSaving model with loss 0.973...\nSaving model with loss 0.965...\nSaving model with loss 0.952...\nSaving model with loss 0.951...\nSaving model with loss 0.935...\nSaving model with loss 0.929...\nSaving model with loss 0.922...\nSaving model with loss 0.917...\nSaving model with loss 0.902...\nSaving model with loss 0.885...\nSaving model with loss 0.884...\nSaving model with loss 0.879...\nSaving model with loss 0.868...\nSaving model with loss 0.866...\nSaving model with loss 0.865...\nSaving model with loss 0.862...\nSaving model with loss 0.859...\nSaving model with loss 0.853...\nSaving model with loss 0.851...\nSaving model with loss 0.848...\nSaving model with loss 0.832...\nSaving model with loss 0.830...\nSaving model with loss 0.828...\nSaving model with loss 0.819...\nSaving model with loss 0.816...\nSaving model with loss 0.810...\nSaving model with loss 0.809...\nSaving model with loss 0.808...\nSaving model with loss 0.802...\nSaving model with loss 0.793...\nSaving model with loss 0.790...\nSaving model with loss 0.788...\nSaving model with loss 0.785...\nSaving model with loss 0.784...\nSaving model with loss 0.784...\nSaving model with loss 0.783...\nSaving model with loss 0.777...\nSaving model with loss 0.769...\nSaving model with loss 0.760...\nSaving model with loss 0.760...\nSaving model with loss 0.754...\nSaving model with loss 0.749...\nSaving model with loss 0.743...\nSaving model with loss 0.743...\nSaving model with loss 0.737...\nSaving model with loss 0.735...\nSaving model with loss 0.733...\nBest loss 0.733...\n\nModel is not improving, so we halt the training session.\nvalid_scores: [0.7332841217517853]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 8/8 [00:00<00:00, 1045.02it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Plot learning curves with `tensorboard` (optional)\n\n`tensorboard` is a tool that allows you to visualize your training progress.\n\nIf this block does not display your learning curve, please wait for few minutes, and re-run this block. It might take some time to load your logging information. ","metadata":{}},{"cell_type":"code","source":"%reload_ext tensorboard\n%tensorboard --logdir=./runs/","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-05-31T06:05:25.485188Z","iopub.execute_input":"2023-05-31T06:05:25.485497Z","iopub.status.idle":"2023-05-31T06:05:31.521078Z","shell.execute_reply.started":"2023-05-31T06:05:25.485472Z","shell.execute_reply":"2023-05-31T06:05:31.520006Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n      <iframe id=\"tensorboard-frame-63c2833c7e77a425\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-63c2833c7e77a425\");\n          const url = new URL(\"/\", window.location);\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "},"metadata":{}}]},{"cell_type":"code","source":"    x_test, test_loader = objective(None)\n    model = My_Model(input_dim=x_test.shape[1]).to(device)\n    model.load_state_dict(torch.load(config['save_path']))\n    preds = predict(test_loader, model, device)\n    save_pred(preds, 'submission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-05-31T06:05:31.522639Z","iopub.execute_input":"2023-05-31T06:05:31.523061Z","iopub.status.idle":"2023-05-31T06:06:03.806191Z","shell.execute_reply.started":"2023-05-31T06:05:31.523027Z","shell.execute_reply":"2023-05-31T06:06:03.805301Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"hyper-parameter: \n        seed: 5201314\n        optimizer: Adam,\n        lr: 0.0001, \n        batch_size: 128, \n        k: 16, \n        layer: [16, 8]\nSaving model with loss 378.212...\nSaving model with loss 374.679...\nSaving model with loss 372.607...\nSaving model with loss 368.441...\nSaving model with loss 365.675...\nSaving model with loss 364.324...\nSaving model with loss 363.158...\nSaving model with loss 358.136...\nSaving model with loss 356.697...\nSaving model with loss 354.515...\nSaving model with loss 350.502...\nSaving model with loss 347.380...\nSaving model with loss 347.309...\nSaving model with loss 344.518...\nSaving model with loss 338.820...\nSaving model with loss 330.098...\nSaving model with loss 325.419...\nSaving model with loss 321.391...\nSaving model with loss 317.663...\nSaving model with loss 313.248...\nSaving model with loss 307.808...\nSaving model with loss 303.925...\nSaving model with loss 298.641...\nSaving model with loss 293.779...\nSaving model with loss 285.515...\nSaving model with loss 278.147...\nSaving model with loss 273.576...\nSaving model with loss 266.367...\nSaving model with loss 260.249...\nSaving model with loss 250.174...\nSaving model with loss 247.875...\nSaving model with loss 240.853...\nSaving model with loss 233.777...\nSaving model with loss 231.483...\nSaving model with loss 226.609...\nSaving model with loss 218.187...\nSaving model with loss 210.722...\nSaving model with loss 204.504...\nSaving model with loss 201.635...\nSaving model with loss 197.347...\nSaving model with loss 192.278...\nSaving model with loss 185.237...\nSaving model with loss 185.134...\nSaving model with loss 176.908...\nSaving model with loss 172.638...\nSaving model with loss 167.432...\nSaving model with loss 165.895...\nSaving model with loss 160.584...\nSaving model with loss 159.676...\nSaving model with loss 156.075...\nSaving model with loss 154.808...\nSaving model with loss 151.503...\nSaving model with loss 150.098...\nSaving model with loss 149.155...\nSaving model with loss 148.862...\nSaving model with loss 145.741...\nBest loss 145.741...\n\nModel is not improving, so we halt the training session.\nvalid_scores: [145.74065093994142]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 8/8 [00:00<00:00, 1153.43it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}